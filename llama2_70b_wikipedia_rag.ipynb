{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31f47465",
      "metadata": {
        "papermill": {
          "duration": 0.008692,
          "end_time": "2023-09-30T05:36:50.159819",
          "exception": false,
          "start_time": "2023-09-30T05:36:50.151127",
          "status": "completed"
        },
        "tags": [],
        "id": "31f47465"
      },
      "source": [
        "# LLama-70B + Wikipedia RAG\n",
        "\n",
        "## Introduction ğŸŒŸ\n",
        "Welcome to this Jupyter notebook developed for Kaggle - LLM Science Exam This notebook is designed to help you participate in the competition and to Use LLMs to answer difficult science questions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef6d933",
      "metadata": {
        "papermill": {
          "duration": 0.007583,
          "end_time": "2023-09-30T05:36:50.175463",
          "exception": false,
          "start_time": "2023-09-30T05:36:50.167880",
          "status": "completed"
        },
        "tags": [],
        "id": "6ef6d933"
      },
      "source": [
        "## ğŸ“¦ Install offline dependencies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ed10367",
      "metadata": {
        "papermill": {
          "duration": 0.007095,
          "end_time": "2023-09-30T05:36:50.190155",
          "exception": false,
          "start_time": "2023-09-30T05:36:50.183060",
          "status": "completed"
        },
        "tags": [],
        "id": "9ed10367"
      },
      "source": [
        "**Explanation:**\n",
        "- In this cell, you are installing two specific Python packages from local files.\n",
        "- The `!pip install` command is used to install Python packages.\n",
        "- The `-U` flag is used to upgrade the packages if they are already installed.\n",
        "- `--no-deps` flag is used to skip installing dependencies since you are installing from local files.\n",
        "- The paths to the `.whl` files are provided after `--no-deps` to specify the location of the packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3786c02",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-09-30T05:36:50.207488Z",
          "iopub.status.busy": "2023-09-30T05:36:50.206670Z",
          "iopub.status.idle": "2023-09-30T05:37:46.407490Z",
          "shell.execute_reply": "2023-09-30T05:37:46.406221Z"
        },
        "papermill": {
          "duration": 56.212506,
          "end_time": "2023-09-30T05:37:46.410017",
          "exception": false,
          "start_time": "2023-09-30T05:36:50.197511",
          "status": "completed"
        },
        "tags": [],
        "id": "f3786c02",
        "outputId": "253d035c-d5cc-4a1e-de77-ab9725167912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
            "Installing collected packages: faiss-gpu\r\n",
            "Successfully installed faiss-gpu-1.7.2\r\n",
            "Processing /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\r\n",
            "Installing collected packages: datasets\r\n",
            "  Attempting uninstall: datasets\r\n",
            "    Found existing installation: datasets 2.1.0\r\n",
            "    Uninstalling datasets-2.1.0:\r\n",
            "      Successfully uninstalled datasets-2.1.0\r\n",
            "Successfully installed datasets-2.14.5\r\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d74d0d8",
      "metadata": {
        "papermill": {
          "duration": 0.004809,
          "end_time": "2023-09-30T05:37:46.420189",
          "exception": false,
          "start_time": "2023-09-30T05:37:46.415380",
          "status": "completed"
        },
        "tags": [],
        "id": "7d74d0d8"
      },
      "source": [
        "## Importing Libraries and Setting Constants\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6775aade",
      "metadata": {
        "papermill": {
          "duration": 0.004613,
          "end_time": "2023-09-30T05:37:46.429692",
          "exception": false,
          "start_time": "2023-09-30T05:37:46.425079",
          "status": "completed"
        },
        "tags": [],
        "id": "6775aade"
      },
      "source": [
        "**Explanation:**\n",
        "- This cell imports various libraries and sets constants that will be used throughout the notebook.\n",
        "- Libraries include standard Python libraries (e.g., `gc`, `logging`, `time`) and popular data science libraries (e.g., `numpy`, `pandas`, `torch`).\n",
        "- Constants like `NUM_TITLES`, `MAX_SEQ_LEN`, and `MODEL_PATH` are defined here for easy adjustment and reference throughout the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cadcdd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:37:46.441077Z",
          "iopub.status.busy": "2023-09-30T05:37:46.440692Z",
          "iopub.status.idle": "2023-09-30T05:38:11.067987Z",
          "shell.execute_reply": "2023-09-30T05:38:11.067088Z"
        },
        "papermill": {
          "duration": 24.635682,
          "end_time": "2023-09-30T05:38:11.070176",
          "exception": false,
          "start_time": "2023-09-30T05:37:46.434494",
          "status": "completed"
        },
        "tags": [],
        "id": "16cadcdd",
        "outputId": "12821dbb-46b6-4992-8c8b-ab34b2b4a9ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import logging\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import ctypes\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# For RAG\n",
        "import faiss\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_from_disk, Dataset\n",
        "\n",
        "NUM_TITLES = 5\n",
        "MAX_SEQ_LEN = 512\n",
        "MODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n",
        "\n",
        "# For LLM\n",
        "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
        "from accelerate import init_empty_weights\n",
        "from accelerate.utils.modeling import set_module_tensor_to_device\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "N_BATCHES = 5\n",
        "MAX_CONTEXT = 2750\n",
        "MAX_LENGTH = 4096"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ecd14e6",
      "metadata": {
        "papermill": {
          "duration": 0.004169,
          "end_time": "2023-09-30T05:38:11.079141",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.074972",
          "status": "completed"
        },
        "tags": [],
        "id": "1ecd14e6"
      },
      "source": [
        " ## ğŸ“‚Function to Clean Memory and Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05000f4d",
      "metadata": {
        "papermill": {
          "duration": 0.00424,
          "end_time": "2023-09-30T05:38:11.087965",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.083725",
          "status": "completed"
        },
        "tags": [],
        "id": "05000f4d"
      },
      "source": [
        "**Explanation:**\n",
        "- This cell defines a function `clean_memory()` to clear RAM and vRAM (video RAM) using garbage collection, memory trimming, and CUDA memory clearing.\n",
        "- Data is loaded from a CSV file into a Pandas DataFrame using `pd.read_csv()`. The file path and index column are specified.\n",
        "- The `IS_TEST_SET` variable controls whether the notebook runs on a full dataset or a smaller subset (for testing purposes).\n",
        "- There's an option to uncomment a block of code to load the train set and adjust `IS_TEST_SET` and `N_BATCHES` accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943573dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.097650Z",
          "iopub.status.busy": "2023-09-30T05:38:11.097342Z",
          "iopub.status.idle": "2023-09-30T05:38:11.140819Z",
          "shell.execute_reply": "2023-09-30T05:38:11.140008Z"
        },
        "papermill": {
          "duration": 0.050568,
          "end_time": "2023-09-30T05:38:11.142625",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.092057",
          "status": "completed"
        },
        "tags": [],
        "id": "943573dd"
      },
      "outputs": [],
      "source": [
        "# Function to clean RAM & vRAM\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", index_col=\"id\")\n",
        "\n",
        "# Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\n",
        "IS_TEST_SET = len(df) != 200\n",
        "\n",
        "# Uncomment this to see results on the train set\n",
        "# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\n",
        "# IS_TEST_SET = True\n",
        "# N_BATCHES = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab895ed7",
      "metadata": {
        "papermill": {
          "duration": 0.004628,
          "end_time": "2023-09-30T05:38:11.151770",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.147142",
          "status": "completed"
        },
        "tags": [],
        "id": "ab895ed7"
      },
      "source": [
        "## ğŸ“¦ SentenceTransformer Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8814d8b",
      "metadata": {
        "papermill": {
          "duration": 0.004034,
          "end_time": "2023-09-30T05:38:11.163300",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.159266",
          "status": "completed"
        },
        "tags": [],
        "id": "b8814d8b"
      },
      "source": [
        "**Explanation:**\n",
        "- This cell defines a custom `SentenceTransformer` class used for encoding sentences into embeddings using a pre-trained model.\n",
        "- The class has an `__init__` method for initializing the model, a `transform` method for preprocessing sentences, a `get_dataloader` method for creating a DataLoader for sentences, and an `encode` method for encoding sentences into embeddings.\n",
        "- In the `__init__` method, the pre-trained model and tokenizer are loaded based on the specified checkpoint.\n",
        "- The `transform` method tokenizes and preprocesses sentences using the tokenizer.\n",
        "- The `get_dataloader` method prepares a DataLoader for a list of sentences.\n",
        "- The `encode` method encodes sentences into embeddings using the loaded model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fe76f4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.172830Z",
          "iopub.status.busy": "2023-09-30T05:38:11.172574Z",
          "iopub.status.idle": "2023-09-30T05:38:11.180359Z",
          "shell.execute_reply": "2023-09-30T05:38:11.179435Z"
        },
        "papermill": {
          "duration": 0.014286,
          "end_time": "2023-09-30T05:38:11.182003",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.167717",
          "status": "completed"
        },
        "tags": [],
        "id": "f0fe76f4"
      },
      "outputs": [],
      "source": [
        "class SentenceTransformer:\n",
        "    def __init__(self, checkpoint, device=\"cuda:0\"):\n",
        "        self.device = device\n",
        "        self.checkpoint = checkpoint\n",
        "        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "    def transform(self, batch):\n",
        "        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n",
        "        return tokens.to(self.device)\n",
        "\n",
        "    def get_dataloader(self, sentences, batch_size=32):\n",
        "        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n",
        "        dataset = Dataset.from_dict({\"text\": sentences})\n",
        "        dataset.set_transform(self.transform)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "        return dataloader\n",
        "\n",
        "    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n",
        "        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n",
        "        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n",
        "\n",
        "        embeddings = []\n",
        "        for batch in pbar:\n",
        "            with torch.no_grad():\n",
        "                e = self.model(**batch).pooler_output\n",
        "                e = F.normalize(e, p=2, dim=1)\n",
        "                embeddings.append(e.detach().cpu().numpy())\n",
        "        embeddings = np.concatenate(embeddings, axis=0)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d99315",
      "metadata": {
        "papermill": {
          "duration": 0.003944,
          "end_time": "2023-09-30T05:38:11.190204",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.186260",
          "status": "completed"
        },
        "tags": [],
        "id": "c3d99315"
      },
      "source": [
        "## Processing and Extracting Context for Test Set**\n",
        "\n",
        "**Explanation:**\n",
        "- This cell processes and extracts context for the test set if `IS_TEST_SET` is `True`.\n",
        "- It starts by loading an embedding model (`SentenceTransformer`) and initializing a timer (`start`) to measure the elapsed time.\n",
        "- The prompts and answer choices are combined into a single string using a lambda function and applied to the DataFrame, resulting in `inputs` for embedding.\n",
        "- A Faiss index is loaded to perform efficient nearest neighbor search for text matching.\n",
        "- Text search is performed using Faiss by searching for the closest sentences to the prompt embeddings.\n",
        "- Context is extracted from a pre-loaded dataset, and each entry in the `df` DataFrame is updated with the extracted context.\n",
        "- Memory is freed, including resetting the Faiss index, deleting variables, and running the `clean_memory()` function.\n",
        "- The elapsed time for the entire process is printed.\n",
        "\n",
        "**Note:**\n",
        "- The Faiss library is used for efficient similarity search, which can significantly speed up the search for relevant passages in a large dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6d96f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.200092Z",
          "iopub.status.busy": "2023-09-30T05:38:11.199572Z",
          "iopub.status.idle": "2023-09-30T05:38:11.206136Z",
          "shell.execute_reply": "2023-09-30T05:38:11.205145Z"
        },
        "papermill": {
          "duration": 0.013598,
          "end_time": "2023-09-30T05:38:11.207901",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.194303",
          "status": "completed"
        },
        "tags": [],
        "id": "6f6d96f6"
      },
      "outputs": [],
      "source": [
        "if IS_TEST_SET:\n",
        "    # Load embedding model\n",
        "    start = time()\n",
        "    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n",
        "    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n",
        "\n",
        "    # Get embeddings of prompts\n",
        "    f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n",
        "    inputs = df.apply(f, axis=1).values # better results than prompt only\n",
        "    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n",
        "\n",
        "    # Search closest sentences in the wikipedia index\n",
        "    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n",
        "    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n",
        "    # faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n",
        "\n",
        "    print(f\"Starting text search, t={time() - start :.1f}s\")\n",
        "    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n",
        "\n",
        "    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n",
        "    dataset = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n",
        "    for i in range(len(df)):\n",
        "        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n",
        "\n",
        "    # Free memory\n",
        "    faiss_index.reset()\n",
        "    del faiss_index, prompt_embeddings, model, dataset\n",
        "    clean_memory()\n",
        "    print(f\"Context added, t={time() - start :.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de6e50e7",
      "metadata": {
        "papermill": {
          "duration": 0.004254,
          "end_time": "2023-09-30T05:38:11.216468",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.212214",
          "status": "completed"
        },
        "tags": [],
        "id": "de6e50e7"
      },
      "source": [
        "##  Creating Symlinks from Kaggle Datasets to Cached Model\n",
        "\n",
        "\n",
        "**Explanation:**\n",
        "- This cell creates symbolic links (symlinks) from dataset files in Kaggle to a cached model checkpoint directory.\n",
        "- It starts by defining the `checkpoint_path`, which is the directory where the symlinks will be created.\n",
        "- The script then loops through two parts (part 1 and part 2).\n",
        "- For each part, it defines the `source_dir`, which is the directory containing dataset files.\n",
        "- It then iterates through files in the `source_dir`.\n",
        "- For each file, it attempts to create a symlink in the `checkpoint_path` directory with the same name, pointing to the original file in the `source_dir`.\n",
        "- Any exceptions that occur during symlink creation are caught and ignored.\n",
        "\n",
        "**Note:**\n",
        "- This code is useful for setting up symlinks between Kaggle dataset files and a cached model directory, potentially reducing the need to download data repeatedly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3232c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.226235Z",
          "iopub.status.busy": "2023-09-30T05:38:11.225556Z",
          "iopub.status.idle": "2023-09-30T05:38:11.265496Z",
          "shell.execute_reply": "2023-09-30T05:38:11.264560Z"
        },
        "papermill": {
          "duration": 0.04658,
          "end_time": "2023-09-30T05:38:11.267145",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.220565",
          "status": "completed"
        },
        "tags": [],
        "id": "1b3232c9"
      },
      "outputs": [],
      "source": [
        "# Create symlinks from kaggle datasets to fake cached model\n",
        "\n",
        "checkpoint_path = Path(\"/root/.cache/\")\n",
        "checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "for part in [1, 2]:\n",
        "    source_dir = Path(f\"/kaggle/input/llama2-70b-instruct-part{part}\")\n",
        "    for path in source_dir.glob(\"*\"):\n",
        "        try:\n",
        "            (checkpoint_path / path.name).symlink_to(path)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d697e532",
      "metadata": {
        "papermill": {
          "duration": 0.004243,
          "end_time": "2023-09-30T05:38:11.275536",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.271293",
          "status": "completed"
        },
        "tags": [],
        "id": "d697e532"
      },
      "source": [
        " ## ğŸ¦™ShardedLlama Class for Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5e6b93e",
      "metadata": {
        "papermill": {
          "duration": 0.003962,
          "end_time": "2023-09-30T05:38:11.283554",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.279592",
          "status": "completed"
        },
        "tags": [],
        "id": "c5e6b93e"
      },
      "source": [
        "**Explaination**:\n",
        "\n",
        "1. `class ShardedLlama:`: Defines a Python class named `ShardedLlama`.\n",
        "\n",
        "2. `def __init__(self, checkpoint_path, device=\"cuda:0\", dtype=torch.float16):`: Defines the constructor for the `ShardedLlama` class, which initializes an instance of the class.\n",
        "\n",
        "  \n",
        "\n",
        " - `checkpoint_path`: The path to the checkpoint.\n",
        "   - `device` (optional): The device to use (default is \"cuda:0\" for GPU).\n",
        "   - `dtype` (optional): The data type for tensors (default is torch.float16).\n",
        "\n",
        "3. Inside the constructor, various attributes are initialized:\n",
        "   - `self.checkpoint_path`: Stores the provided checkpoint path as a `Path` object.\n",
        "   - `self.device`: Stores the specified device for computation.\n",
        "   - `self.dtype`: Stores the specified data type for tensors.\n",
        "   - `self.config`: Loads the configuration from the specified checkpoint path using `AutoConfig`.\n",
        "   - `self.tokenizer`: Initializes a tokenizer using `AutoTokenizer` from the checkpoint.\n",
        "   - Sets the tokenizer's `pad_token` to the tokenizer's `eos_token` and sets `padding_side` to \"right.\"\n",
        "   - Calls the `init_model` method to initialize the model.\n",
        "\n",
        "4. `def init_model(self):`: Defines a method for initializing the model.\n",
        "\n",
        "   - Loads the meta-model (without using memory) using the `init_empty_weights()` context manager.\n",
        "   - Initializes the model using `AutoModelForCausalLM.from_config`.\n",
        "   - Ties the model's weights.\n",
        "   - Stores the layers of the model in `self.layers`.\n",
        "   - Moves model buffers to the specified device.\n",
        "\n",
        "5. `def load_layer(self, layer_name):`: Defines a method for loading a specific layer of the model.\n",
        "\n",
        "   - Loads the state dictionary from a file based on the provided `layer_name` and device.\n",
        "   - Iterates through the parameters in the state dictionary and moves them to the specified device.\n",
        "\n",
        "6. `def __call__(self, inputs, output_token):`: Defines the `__call__` method to make instances of `ShardedLlama` callable.\n",
        "\n",
        "   - `inputs` is a list of tuples, where each tuple contains a prefix and suffix.\n",
        "   - `output_token` is an index used to select an output token.\n",
        "\n",
        "7. Inside the `__call__` method:\n",
        "   - The model is rebooted to ensure that buffers are loaded, and memory is clean.\n",
        "   - The input batch is sent to the specified device.\n",
        "   - `n_suffixes` is the number of suffixes in the batch, and `suffix_eos` calculates the position of the end-of-sequence token in each suffix.\n",
        "\n",
        "8. Attention mask and position IDs are created for model inputs.\n",
        "\n",
        "9. The code uses a `ThreadPoolExecutor` to parallelize the loading of model layers.\n",
        "\n",
        "10. For each layer:\n",
        "    - It loads the current layer and waits for the previous layer to be loaded.\n",
        "    - For each input in the batch:\n",
        "      - If it's the \"model.embed_tokens\" layer, the prefix and suffix are passed through this layer.\n",
        "      - If it's the \"model.norm\" layer, only the last token in each suffix is kept.\n",
        "      - If it's the \"lm_head\" layer, predictions are made for the specified output token.\n",
        "      - For other layers, both prefix and suffix are processed, and intermediate results are stored.\n",
        "\n",
        "11. After processing each layer, the previous layer is removed from memory.\n",
        "\n",
        "12. The final batch of results is returned.\n",
        "\n",
        "This `ShardedLlama` class is designed for language modeling, where the model is split into layers and each layer is loaded sequentially to optimize memory usage. The `__call__` method takes a batch of inputs and processes them through the model, providing the results for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763068ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.293866Z",
          "iopub.status.busy": "2023-09-30T05:38:11.293222Z",
          "iopub.status.idle": "2023-09-30T05:38:11.309231Z",
          "shell.execute_reply": "2023-09-30T05:38:11.308407Z"
        },
        "papermill": {
          "duration": 0.023196,
          "end_time": "2023-09-30T05:38:11.311008",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.287812",
          "status": "completed"
        },
        "tags": [],
        "id": "763068ff"
      },
      "outputs": [],
      "source": [
        "# Class for sharded llama\n",
        "\n",
        "class ShardedLlama:\n",
        "    def __init__(self, checkpoint_path, device=\"cuda:0\", dtype=torch.float16):\n",
        "        \"\"\"\n",
        "        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n",
        "        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n",
        "        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n",
        "        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        checkpoint_path : str or Path\n",
        "            path to the checkpoint\n",
        "        device : str, optional\n",
        "            device, by default \"cuda:0\"\n",
        "        dtype : torch.dtype, optional\n",
        "            dtype, by default torch.float16\n",
        "        \"\"\"\n",
        "\n",
        "        # Save parameters\n",
        "        self.checkpoint_path = Path(checkpoint_path)\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # Create model\n",
        "        self.config = AutoConfig.from_pretrained(self.checkpoint_path)\n",
        "        # For flash attention when Turing architecture will be supported : https://github.com/Dao-AILab/flash-attention/issues/542\n",
        "        # self.config.auto_map = {\"AutoModelForCausalLM\" : \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"}\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "        self.init_model()\n",
        "        self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"lm_head\"]\n",
        "\n",
        "    def init_model(self):\n",
        "\n",
        "        # Load meta model (no memory used)\n",
        "        with init_empty_weights():\n",
        "            self.model = AutoModelForCausalLM.from_config(self.config, trust_remote_code=True)\n",
        "            self.model.tie_weights()\n",
        "\n",
        "        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n",
        "\n",
        "        # Move buffers to device (not that much GPU memory used)\n",
        "        for buffer_name, buffer in self.model.named_buffers():\n",
        "            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n",
        "\n",
        "    def load_layer(self, layer_name):\n",
        "        state_dict = load_file(self.checkpoint_path / (layer_name + \".safetensors\"), device=self.device)\n",
        "        for param_name, param in state_dict.items():\n",
        "            assert param.dtype != torch.int8, \"int8 not supported (need to add fp16_statistics)\"\n",
        "            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n",
        "\n",
        "    def __call__(self, inputs, output_token):\n",
        "        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n",
        "\n",
        "        # Reboot the model to make sure buffers are loaded and memory is clean\n",
        "        del self.model\n",
        "        clean_memory()\n",
        "        self.init_model()\n",
        "\n",
        "       # Send batch to device\n",
        "        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n",
        "        n_suffixes = len(batch[0][1])\n",
        "        suffix_eos = [(suffix != self.tokenizer.pad_token_id).sum(1) - 1 for _, suffix in inputs]\n",
        "\n",
        "        # Create attention mask for the largest input, and position ids to use KV cache\n",
        "        attention_mask = torch.finfo(self.dtype).min * torch.ones(MAX_LENGTH, MAX_LENGTH)\n",
        "        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...]\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n",
        "\n",
        "        with ThreadPoolExecutor() as executor, torch.inference_mode():\n",
        "\n",
        "            # Load first layer\n",
        "            #future = executor.submit(self.load_layer, \"model.embed_tokens\")\n",
        "            self.load_layer(\"model.embed_tokens\")\n",
        "\n",
        "            for i, (layer_name, layer) in tqdm(enumerate(zip(self.layer_names, self.layers)), desc=self.device, total=len(self.layers)):\n",
        "\n",
        "                # Wait for previous layer to be loaded and load next layer\n",
        "                #future.result()\n",
        "                if (i + 1) < len(self.layer_names):\n",
        "                    #future = executor.submit(self.load_layer, self.layer_names[i + 1])\n",
        "                    self.load_layer(self.layer_names[i + 1])\n",
        "\n",
        "                # Run layer\n",
        "                for j, (prefix, suffix) in enumerate(batch):\n",
        "                    if layer_name == \"model.embed_tokens\":\n",
        "                        batch[j] = (layer(prefix), layer(suffix))\n",
        "                    elif layer_name == \"model.norm\":\n",
        "                        # Only keep the last token at this point\n",
        "                        batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n",
        "                    elif layer_name == \"lm_head\":\n",
        "                        batch[j] = layer(suffix)[:, 0, output_token].detach().cpu().numpy()\n",
        "                    else:\n",
        "                        # Run prefix\n",
        "                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n",
        "                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n",
        "\n",
        "                        # Run suffix\n",
        "                        pos = position_ids[:, len_p:len_p + len_s].repeat(n_suffixes, 1)\n",
        "                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].repeat(n_suffixes, 1, 1, 1)\n",
        "                        kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.repeat(n_suffixes, 1, 1, 1))\n",
        "                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n",
        "                        batch[j] = (new_prefix, new_suffix)\n",
        "\n",
        "                # Remove previous layer from memory (including buffers)\n",
        "                layer.to(\"meta\")\n",
        "                clean_memory() # proposed by CPMP\n",
        "\n",
        "        # Get scores\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496d0211",
      "metadata": {
        "papermill": {
          "duration": 0.004579,
          "end_time": "2023-09-30T05:38:11.320117",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.315538",
          "status": "completed"
        },
        "tags": [],
        "id": "496d0211"
      },
      "source": [
        "## Running the Model on Multiple GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29ece37",
      "metadata": {
        "papermill": {
          "duration": 0.004195,
          "end_time": "2023-09-30T05:38:11.328452",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.324257",
          "status": "completed"
        },
        "tags": [],
        "id": "d29ece37"
      },
      "source": [
        "\n",
        "```python\n",
        "# Define a function to get tokens for the model input\n",
        "def get_tokens(row, tokenizer):\n",
        "    system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_prefix}\"\n",
        "    instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n",
        "    input_prefix = f\"Context: {row['context'][:MAX_CONTEXT]}\\nQuestion: {row['prompt']}\\nProposed answer: \"\n",
        "    prompt_prefix = system_prefix.format(instruction=instruction, input_prefix=input_prefix)\n",
        "    prefix = tokenizer(prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n",
        "    prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n",
        "    suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n",
        "    return prefix, suffix\n",
        "```\n",
        "\n",
        "- This code defines a function named `get_tokens` that takes two arguments: `row` and `tokenizer`.\n",
        "- `system_prefix` contains a template for a system prefix, which is a text block describing the task and input context.\n",
        "- `instruction` is a string with instructions for the task.\n",
        "- `input_prefix` is generated using information from the input data `row`.\n",
        "- `prompt_prefix` combines `instruction` and `input_prefix` by formatting the `system_prefix` template with the provided values.\n",
        "- `prefix` is generated by tokenizing `prompt_prefix` using the `tokenizer`, converting it to PyTorch tensors, and extracting the input IDs.\n",
        "- `prompt_suffix` is a list of strings generated for each answer choice A, B, C, D, and E.\n",
        "- `suffix` is generated similarly to `prefix` but for `prompt_suffix`, and the padding token is removed from the start.\n",
        "\n",
        "```python\n",
        "# Define a function to run the model on a device\n",
        "def run_model(device, df):\n",
        "    model = ShardedLlama(checkpoint_path, device=f\"cuda:{device}\")\n",
        "    f = partial(get_tokens, tokenizer=model.tokenizer)\n",
        "    inputs = df.apply(f, axis=1).values\n",
        "    batches = np.array_split(inputs, N_BATCHES)\n",
        "    outputs = []\n",
        "    for i, batch in enumerate(batches):\n",
        "        # Token #4874 is yes.\n",
        "        outputs += model(batch, output_token=4874)\n",
        "    return outputs\n",
        "```\n",
        "\n",
        "- This code defines a function named `run_model` that takes two arguments: `device` and `df` (DataFrame).\n",
        "- Inside the function, a `model` is initialized using the `ShardedLlama` class, specifying the GPU device.\n",
        "- A partial function `f` is created using `get_tokens` and the model's tokenizer. This partial function will be used to process inputs.\n",
        "- Inputs for the model are generated by applying the `f` function to each row of the DataFrame `df`.\n",
        "- The inputs are split into batches using `np.array_split`.\n",
        "- The function initializes an empty list called `outputs` to collect model outputs.\n",
        "- It then iterates through the batches, running the model on each batch and appending the results to the `outputs` list.\n",
        "\n",
        "```python\n",
        "# Run model\n",
        "if IS_TEST_SET:\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        outputs = list(executor.map(run_model, [0, 1], np.array_split(df, 2)))\n",
        "        outputs = sum(outputs, [])\n",
        "        \n",
        "    # Save results\n",
        "    n = len(df)\n",
        "    for i, scores in enumerate(outputs):\n",
        "        top3 = np.argsort(scores)[::-1]\n",
        "        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n",
        "    \n",
        "    # Display performances if train set is used (in this case use IS_TEST_SET=True !)\n",
        "    if \"answer\" in df.columns:\n",
        "        for i in range(n):\n",
        "            df.loc[i, \"top_1\"] = df.loc[i, \"prediction\"][0]\n",
        "            df.loc[i, \"top_2\"] = df.loc[i, \"prediction\"][2]\n",
        "            df.loc[i, \"top_3\"] = df.loc[i, \"prediction\"][4]\n",
        "\n",
        "        top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
        "        print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n",
        "        print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")\n",
        "else:\n",
        "    df[\"prediction\"] = \"A B C\"\n",
        "\n",
        "df[[\"prediction\"]].to_csv(\"submission.csv\")\n",
        "```\n",
        "\n",
        "- This part of the code is conditional on the value of `IS_TEST_SET`. If it's `True`, the model is run\n",
        "\n",
        " on the test set.\n",
        "- A `ThreadPoolExecutor` is used to run the `run_model` function on two GPUs concurrently.\n",
        "- The outputs from the model are collected and combined into a single list.\n",
        "- The results are saved back to the DataFrame `df`, and for each row, the top 3 predictions are determined and stored.\n",
        "- If the DataFrame contains an \"answer\" column, performance metrics such as accuracy and mean average precision (map3) are calculated and printed.\n",
        "- If `IS_TEST_SET` is `False`, a default prediction of \"A B C\" is assigned to each row in the DataFrame.\n",
        "- Finally, the DataFrame's \"prediction\" column is saved to a CSV file named \"submission.csv.\"\n",
        "\n",
        "This code effectively runs the language model on the given data and produces predictions or evaluation metrics depending on the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65dc390",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-30T05:38:11.338334Z",
          "iopub.status.busy": "2023-09-30T05:38:11.338106Z",
          "iopub.status.idle": "2023-09-30T05:38:11.355969Z",
          "shell.execute_reply": "2023-09-30T05:38:11.355153Z"
        },
        "papermill": {
          "duration": 0.02499,
          "end_time": "2023-09-30T05:38:11.357649",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.332659",
          "status": "completed"
        },
        "tags": [],
        "id": "c65dc390"
      },
      "outputs": [],
      "source": [
        "# Run model on the 2 GPUs\n",
        "\n",
        "def get_tokens(row, tokenizer):\n",
        "        system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_prefix}\"\n",
        "        instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n",
        "        input_prefix = f\"Context: {row['context'][:MAX_CONTEXT]}\\nQuestion: {row['prompt']}\\nProposed answer: \"\n",
        "        prompt_prefix = system_prefix.format(instruction=instruction, input_prefix=input_prefix)\n",
        "        prefix = tokenizer(prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n",
        "        prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n",
        "        suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n",
        "        return prefix, suffix\n",
        "\n",
        "def run_model(device, df):\n",
        "    model = ShardedLlama(checkpoint_path, device=f\"cuda:{device}\")\n",
        "    f = partial(get_tokens, tokenizer=model.tokenizer)\n",
        "    inputs = df.apply(f, axis=1).values\n",
        "    batches = np.array_split(inputs, N_BATCHES)\n",
        "    outputs = []\n",
        "    for i, batch in enumerate(batches):\n",
        "        # Token #4874 is yes.\n",
        "        outputs += model(batch, output_token=4874)\n",
        "    return outputs\n",
        "\n",
        "# Run model\n",
        "if IS_TEST_SET:\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        outputs = list(executor.map(run_model, [0, 1], np.array_split(df, 2)))\n",
        "        outputs = sum(outputs, [])\n",
        "\n",
        "    # Save results\n",
        "    n = len(df)\n",
        "    for i, scores in enumerate(outputs):\n",
        "        top3 = np.argsort(scores)[::-1]\n",
        "        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n",
        "\n",
        "    # Display performances if train set is used (in this case use IS_TEST_SET=True !)\n",
        "    if \"answer\" in df.columns:\n",
        "        for i in range(n):\n",
        "            df.loc[i, \"top_1\"] = df.loc[i, \"prediction\"][0]\n",
        "            df.loc[i, \"top_2\"] = df.loc[i, \"prediction\"][2]\n",
        "            df.loc[i, \"top_3\"] = df.loc[i, \"prediction\"][4]\n",
        "\n",
        "        top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
        "        print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n",
        "        print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")\n",
        "else:\n",
        "    df[\"prediction\"] = \"A B C\"\n",
        "\n",
        "df[[\"prediction\"]].to_csv(\"submission.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9fa36e5",
      "metadata": {
        "papermill": {
          "duration": 0.004125,
          "end_time": "2023-09-30T05:38:11.365923",
          "exception": false,
          "start_time": "2023-09-30T05:38:11.361798",
          "status": "completed"
        },
        "tags": [],
        "id": "b9fa36e5"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 89.386206,
      "end_time": "2023-09-30T05:38:14.916579",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-09-30T05:36:45.530373",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}