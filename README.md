# [LLM Science Exam](https://www.kaggle.com/competitions/kaggle-llm-science-exam)

Inspired from the [OpenBookQA](https://allenai.org/data/open-book-qa) dataset this competition challenges participants to answer difficult science-based questions written by a Large Language Model.

To make it simple the idea was that the dataset for this Kaggle competition was created from GPT-3.5 which is a 175 billion parameter model. It would be amazing if an LLM which is smaller than its size probably in the range of 7-30 Billion parameters using quantization techniques like QLORA and LORA could solve the exam setup by another LLM.
