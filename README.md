# llm_science_exam

Inspired from [OpenBookQA](https://allenai.org/data/open-book-qa) dataset this competition challenges participants to answer difficult science-based questions written by a Large Language Model.

To make it simple the idea was that the dataset for this Kaggle competiton was created from GPT-3.5 which is 175 billion parameter model. It would be amazing if a LLM which is smaller than its size probably in the range of 7-30 Billion parameters using quantisation techniques like QLORA and LORA can colve the exam setup by another LLM.
